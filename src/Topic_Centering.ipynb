{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "import spacy\n",
    "import numpy as np\n",
    "from nltk import grammar, parse\n",
    "from collections import Counter \n",
    "from cleantext import clean\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/test2.txt'\n",
    "f = open(filename, \"r\")\n",
    "contents = f.read()\n",
    "\n",
    "clean(\"some input\",\n",
    "    fix_unicode=True,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=False,                  # replace all URLs with a special token\n",
    "    no_emails=False,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "    no_numbers=False,               # replace all numbers with a special token\n",
    "    no_digits=False,                # replace all digits with a special token\n",
    "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "    no_punct=False,                 # fully remove punctuation\n",
    "    replace_with_url=\"\",\n",
    "    replace_with_email=\"\",\n",
    "    replace_with_phone_number=\"\",\n",
    "    replace_with_number=\"\",\n",
    "    replace_with_digit=\"\",\n",
    "    replace_with_currency_symbol=\"\",\n",
    "    lang=\"en\"                       # set to 'de' for German special handling\n",
    ")\n",
    "\n",
    "contents = re.sub(r'\\d+', '', contents)\n",
    "contents = re.sub(r\"\"\"\n",
    "               [,;@#?!&$\\\"\\'\\[\\]]+  # Accept one or more copies of punctuation\n",
    "               \\ *           # plus zero or more copies of a space,\n",
    "               \"\"\",\n",
    "               \" \",          # and replace it with a single space\n",
    "               contents, flags=re.VERBOSE)\n",
    "contents = re.sub(' +', ' ', contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = []\n",
    "split_utterances = []\n",
    "sentences = contents.strip().split('.')\n",
    "for sentence in sentences:\n",
    "    if sentence != '':\n",
    "        utterances.append(sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('John', 'NNP'), ('went', 'VBD'), ('to', 'IN'), ('his', 'PRP$'), ('favorite', 'JJ'), ('music', 'NN'), ('store', 'NN'), ('to', 'TO'), ('buy', 'VB'), ('a', 'DT'), ('piano', 'NN')], [('He', 'PRP'), ('was', 'VBD'), ('excited', 'JJ'), ('that', 'IN'), ('he', 'PRP'), ('could', 'MD'), ('finally', 'RB'), ('buy', 'VB'), ('a', 'DT'), ('piano', 'NN')], [('He', 'PRP'), ('arrived', 'VBD'), ('just', 'RB'), ('as', 'IN'), ('the', 'DT'), ('store', 'NN'), ('was', 'VBD'), ('closing', 'VBG'), ('for', 'IN'), ('the', 'DT'), ('day', 'NN')], [('It', 'PRP'), ('was', 'VBD'), ('closing', 'VBG'), ('just', 'RB'), ('as', 'IN'), ('John', 'NNP'), ('arrived', 'VBD')]]\n"
     ]
    }
   ],
   "source": [
    "## pos tags\n",
    "pos_tags_utterances = []\n",
    "\n",
    "# for utterance in utterances:\n",
    "#     pos_tags_utterances.append(pos_tag(utterance.split(' ')))\n",
    "\n",
    "for utterance in utterances:\n",
    "    doc = nlp(utterance)\n",
    "    temp_list = []\n",
    "    for token in doc:\n",
    "        temp_list.append((token.text, token.tag_))\n",
    "    pos_tags_utterances.append(temp_list)\n",
    "print(pos_tags_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic of first sentence\n",
    "processed_utterance = nlp(utterances[0])\n",
    "init_topic = ''\n",
    "for token in processed_utterance:\n",
    "    if \"NN\" in token.tag_:\n",
    "        init_topic = token.text\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['undefined', ('john', 'NNP'), ('john', 'NNP'), ('store', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Centers of \n",
    "total_utterances = len(utterances)\n",
    "tagged_utterances = utterances\n",
    "\n",
    "Cb = [None] * total_utterances\n",
    "\n",
    "Cf = []\n",
    "for i in range(total_utterances):\n",
    "    Cf.append([])\n",
    "\n",
    "Cb[0] = \"undefined\"\n",
    "for i in range(total_utterances):\n",
    "    for token in pos_tags_utterances[i]:\n",
    "        if \"NN\" in token[1]:\n",
    "            Cf[i].append((token[0].lower(), token[1]))\n",
    "        if i!=0:\n",
    "            if \"PRP\" == token[1]:\n",
    "                if token[0].lower() == \"he\" or token[0].lower() == \"she\":\n",
    "                    for pos in Cf[i-1]:\n",
    "                        if pos[1] == \"NNP\":\n",
    "                            if (pos[0].lower(), \"NNP\") not in Cf[i]:\n",
    "                                Cf[i].append((pos[0].lower(), \"NNP\"))\n",
    "                                break\n",
    "                elif token[0].lower() == \"it\":\n",
    "                    for pos in Cf[i-1]:\n",
    "                        if pos[1] == \"NN\":\n",
    "                            if (pos[0].lower(), \"NN\") not in Cf[i]:\n",
    "                                Cf[i].append((pos[0].lower(), \"NN\"))\n",
    "                                break\n",
    "                elif token[0].lower() == 'they':\n",
    "                    for pos in Cf[i-1]:\n",
    "                        if pos[1] == \"NNS\":\n",
    "                            if (pos[0].lower(), \"NN\") not in Cf[i]:\n",
    "                                Cf[i].append((pos[0].lower(), \"NNS\"))\n",
    "                                break\n",
    "\n",
    "    if i != 0:\n",
    "        try:\n",
    "            Cb[i] = Cf[i][0]\n",
    "        except:\n",
    "            Cb[i] = Cb[i-1]\n",
    "print(Cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'John', 'John', 'store']\n"
     ]
    }
   ],
   "source": [
    "topics_utterances = []\n",
    "focus_utterances = []\n",
    "topics_utterances.append(init_topic)\n",
    "for tuple_val in Cb:\n",
    "    if tuple_val != \"undefined\":\n",
    "        if tuple_val[1] == \"NNP\":\n",
    "            topics_utterances.append(tuple_val[0].capitalize())\n",
    "        else:\n",
    "            topics_utterances.append(tuple_val[0])\n",
    "print(topics_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics present in the discourse are:\n",
      "john\n",
      "store\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "for topic in topics_utterances:\n",
    "    if len(topic) != 1:\n",
    "        if topic.lower() not in topic_dict.keys():\n",
    "            topic_dict[topic.lower()] = 1\n",
    "        else:\n",
    "            topic_dict[topic.lower()] += 1\n",
    "        \n",
    "k  = Counter(topic_dict)\n",
    "if len(topic_dict)//2 < 3:\n",
    "    top_topics_num = len(topic_dict)//2 + 1\n",
    "else:\n",
    "    top_topics_num = 3\n",
    "top_topics_list = k.most_common(top_topics_num)\n",
    "\n",
    "\n",
    "print(\"Topics present in the discourse are:\")\n",
    "for topic in top_topics_list:\n",
    "    print(topic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic of every sentence:\n",
      "John went to his favorite music store to buy a piano : John\n",
      "He was excited that he could finally buy a piano : John\n",
      "He arrived just as the store was closing for the day : John\n",
      "It was closing just as John arrived : store\n"
     ]
    }
   ],
   "source": [
    "print(\"Topic of every sentence:\")\n",
    "for utterance, utterance_topic in zip(utterances, topics_utterances):\n",
    "    print(utterance, \":\", utterance_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John went to his favorite music store to buy a piano : [ John ;  went to his favorite music store to buy a piano ]\n",
      "He was excited that he could finally buy a piano : [ john ;  was excited that  could finally buy a piano ]\n",
      "He arrived just as the store was closing for the day : [ john ;  arrived just as t store was closing for t day ]\n",
      "It was closing just as John arrived : [ store ;  was closing just as john arrived ]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "topic_sentence = []\n",
    "focus_sentence = []\n",
    "for utterance, utterance_topic in zip(utterances, topics_utterances):\n",
    "    processed_utterance = nlp(utterance)\n",
    "    topic_chunk = ''\n",
    "    focus_chunk = ''\n",
    "    for chunk in processed_utterance.noun_chunks:\n",
    "        if chunk.root.dep_ == \"nsubj\":\n",
    "            if utterance_topic in chunk.text:\n",
    "                topic_chunk = chunk.text\n",
    "                focus_chunk = utterance.lower().replace(topic_chunk.lower(), \"\")\n",
    "                break\n",
    "            elif \"he\" in chunk.text.lower():\n",
    "                topic_chunk = Cb[i][0]\n",
    "                focus_chunk = utterance.lower().replace(\"he\", \"\")\n",
    "                break\n",
    "            elif \"she\" in chunk.text.lower():\n",
    "                topic_chunk = Cb[i][0]\n",
    "                focus_chunk = utterance.lower().replace(\"she\", \"\")\n",
    "                break\n",
    "            elif \"it\" in chunk.text.lower():\n",
    "                topic_chunk = Cb[i][0]\n",
    "                focus_chunk = utterance.lower().replace(\"it\", \"\")\n",
    "                break\n",
    "            elif \"they\" in chunk.text.lower():\n",
    "                topic_chunk = Cb[i][0]\n",
    "                focus_chunk = utterance.lower().replace(\"they\", \"\")\n",
    "                break\n",
    "    i += 1\n",
    "    topic_sentence.append(topic_chunk)\n",
    "    focus_sentence.append(focus_chunk)\n",
    "\n",
    "for utterance, topic, focus in zip(utterances, topic_sentence, focus_sentence):\n",
    "    print(utterance, \": [\", topic, \";\", focus, \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
