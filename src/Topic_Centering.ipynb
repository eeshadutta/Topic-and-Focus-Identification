{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "import spacy\n",
    "import numpy as np\n",
    "from nltk import grammar, parse\n",
    "from collections import Counter \n",
    "from cleantext import clean\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/test1.txt'\n",
    "f = open(filename, \"r\")\n",
    "contents = f.read()\n",
    "\n",
    "clean(\"some input\",\n",
    "    fix_unicode=True,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=False,                  # replace all URLs with a special token\n",
    "    no_emails=False,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "    no_numbers=False,               # replace all numbers with a special token\n",
    "    no_digits=False,                # replace all digits with a special token\n",
    "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "    no_punct=False,                 # fully remove punctuation\n",
    "    replace_with_url=\"\",\n",
    "    replace_with_email=\"\",\n",
    "    replace_with_phone_number=\"\",\n",
    "    replace_with_number=\"\",\n",
    "    replace_with_digit=\"\",\n",
    "    replace_with_currency_symbol=\"\",\n",
    "    lang=\"en\"                       # set to 'de' for German special handling\n",
    ")\n",
    "\n",
    "utterances = []\n",
    "split_utterances = []\n",
    "sentences = contents.strip().split('.')\n",
    "for sentence in sentences:\n",
    "    if sentence != '':\n",
    "        utterances.append(sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pos tags\n",
    "pos_tags_utterances = []\n",
    "\n",
    "for utterance in utterances:\n",
    "    pos_tags_utterances.append(pos_tag(utterance.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic of first sentence\n",
    "processed_utterance = nlp(utterances[0])\n",
    "init_topic = ''\n",
    "for chunk in processed_utterance.noun_chunks:\n",
    "    if chunk.root.dep_ == \"nsubj\":\n",
    "        init_topic = chunk.text\n",
    "#     print(chunk.text,\",\",chunk.root.text, \",\",chunk.root.dep_,\",\",chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centers of \n",
    "total_utterances = len(utterances)\n",
    "\n",
    "Cb = [None] * total_utterances\n",
    "\n",
    "Cf = []\n",
    "for i in range(total_utterances):\n",
    "    Cf.append([])\n",
    "\n",
    "Cb[0] = \"undefined\"\n",
    "for i in range(total_utterances):\n",
    "    \n",
    "    for token in pos_tags_utterances[i]:\n",
    "        if \"NN\" in token[1]:\n",
    "            Cf[i].append((token[0].lower(), token[1]))\n",
    "        if i!=0:\n",
    "            if \"PRP\" == token[1]:\n",
    "                if token[0].lower() == \"he\" or token[0].lower() == \"she\":\n",
    "                    for pos in Cf[i-1]:\n",
    "                        if pos[1] == \"NNP\":\n",
    "                            if (pos[0].lower(), \"NNP\") not in Cf[i]:\n",
    "                                Cf[i].append((pos[0].lower(), \"NNP\"))\n",
    "                                break\n",
    "                elif token[0].lower() == \"it\":\n",
    "                    for pos in Cf[i-1]:\n",
    "                        if pos[1] == \"NN\":\n",
    "                            if (pos[0].lower(), \"NN\") not in Cf[i]:\n",
    "                                Cf[i].append((pos[0].lower(), \"NN\"))\n",
    "                                break\n",
    "                elif token[0].lower() == 'they':\n",
    "                    for pos in Cf[i-1]:\n",
    "                        if pos[1] == \"NNS\":\n",
    "                            if (pos[0].lower(), \"NN\") not in Cf[i]:\n",
    "                                Cf[i].append((pos[0].lower(), \"NNS\"))\n",
    "                                break\n",
    "\n",
    "    if i != 0:\n",
    "        try:\n",
    "            Cb[i] = Cf[i][0]\n",
    "        except:\n",
    "            Cb[i] = Cb[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_utterances = []\n",
    "focus_utterances = []\n",
    "topics_utterances.append(init_topic)\n",
    "for tuple_val in Cb:\n",
    "    if tuple_val != \"undefined\":\n",
    "        if tuple_val[1] == \"NNP\":\n",
    "            topics_utterances.append(tuple_val[0].capitalize())\n",
    "        else:\n",
    "            topics_utterances.append(tuple_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics present in the discourse are:\n",
      "John\n",
      "Mike\n"
     ]
    }
   ],
   "source": [
    "topic_dict = {}\n",
    "for topic in topics_utterances:\n",
    "    if topic not in topic_dict.keys():\n",
    "        topic_dict[topic] = 1\n",
    "    else:\n",
    "        topic_dict[topic] += 1\n",
    "        \n",
    "k  = Counter(topic_dict)\n",
    "top_topics_num = len(topic_dict)//2\n",
    "top_topics_num = 4\n",
    "top_topics_list = k.most_common(top_topics_num)\n",
    "\n",
    "\n",
    "print(\"Topics present in the discourse are:\")\n",
    "for topic in top_topics_list:\n",
    "    print(topic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic of every sentence:\n",
      "John has been having a lot of trouble arranging his vacation : John\n",
      "He cannot find anyone to take over his responsibilities : John\n",
      "He called up Mike yesterday to work out a plan : John\n",
      "Mike has annoyed him a lot recently : Mike\n",
      "He called John on Friday last week : Mike\n"
     ]
    }
   ],
   "source": [
    "print(\"Topic of every sentence:\")\n",
    "for utterance, utterance_topic in zip(utterances, topics_utterances):\n",
    "    print(utterance, \":\", utterance_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for utterance, utterance_topic in zip(utterances, topics_utterances):\n",
    "    processed_utterance = nlp(utterance)\n",
    "    init_topic = ''\n",
    "    for chunk in processed_utterance.noun_chunks:\n",
    "        if chunk.root.dep_ == \"nsubj\":\n",
    "            init_topic = chunk.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
